{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xt6fIDownZs"
      },
      "source": [
        "## Deep Deterministic Policy Gradient (DDPG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RqrzokqoanP"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "!apt-get install -y xvfb\n",
        "\n",
        "!pip install gym==0.23 \\\n",
        "    pytorch-lightning==1.6.0 \\\n",
        "    pyvirtualdisplay \\\n",
        "    brax==0.10.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOSJl-X7zvs4"
      },
      "source": [
        "#### Setup virtual display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-Z6takfzqGk"
      },
      "outputs": [],
      "source": [
        "from pyvirtualdisplay import Display\n",
        "Display(visible=False, size=(1400, 900)).start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cz8DLleGz_TF"
      },
      "source": [
        "#### Import the necessary code libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cP5t6U7-nYoc"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import gym\n",
        "import torch\n",
        "import random\n",
        "import functools\n",
        "\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from collections import deque, namedtuple\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import IterableDataset\n",
        "from torch.optim import AdamW\n",
        "\n",
        "from pytorch_lightning import LightningModule, Trainer\n",
        "\n",
        "import brax\n",
        "from brax import envs\n",
        "from brax.envs.wrappers import gym as gym_wrapper\n",
        "from brax.envs.wrappers import torch as torch_wrapper\n",
        "from brax.io import html\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "num_gpus = torch.cuda.device_count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_IrPlU1wwPx"
      },
      "outputs": [],
      "source": [
        "def display_video(episode=0):\n",
        "  video_file = open(f'/content/videos/rl-video-episode-{episode}.mp4', \"r+b\").read()\n",
        "  video_url = f\"data:video/mp4;base64,{b64encode(video_file).decode()}\"\n",
        "  return HTML(f\"<video width=600 controls><source src='{video_url}'></video>\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evrLpUqXKres"
      },
      "outputs": [],
      "source": [
        "def create_environment(env_name, num_envs=256, episode_length=1000):\n",
        "    env = envs.create(env_name, batch_size=num_envs, episode_length=episode_length, backend='spring')\n",
        "    env = gym_wrapper.VectorGymWrapper(env)\n",
        "    env = torch_wrapper.TorchWrapper(env, device=device)\n",
        "    return env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwYlpTOY1Ajo"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def test_env(env_name, policy=None):\n",
        "  env = envs.create(env_name, episode_length=1000, backend='spring')\n",
        "  env = gym_wrapper.GymWrapper(env)\n",
        "  env = torch_wrapper.TorchWrapper(env, device=device)\n",
        "  ps_array = []\n",
        "  state = env.reset()\n",
        "  for i in range(1000):\n",
        "    if policy:\n",
        "      action = algo.policy.net(state.unsqueeze(0)).squeeze()\n",
        "    else:\n",
        "      action = torch.from_numpy(env.action_space.sample()).to(device)\n",
        "    state, _, _, _ = env.step(action)\n",
        "    ps_array.extend([env.unwrapped._state.pipeline_state]*5)\n",
        "  return HTML(html.render(env.unwrapped._env.sys, ps_array))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_env('ant')"
      ],
      "metadata": {
        "id": "ZE3exZS837w9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SmWkjyfs7kc"
      },
      "source": [
        "#### Create the gradient policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNDbTLeZuP1m"
      },
      "outputs": [],
      "source": [
        "class GradientPolicy(nn.Module):\n",
        "\n",
        "  def __init__(self, hidden_size, obs_size, out_dims, min, max):\n",
        "    super().__init__()\n",
        "    self.min = torch.from_numpy(min).to(device)\n",
        "    self.max = torch.from_numpy(max).to(device)\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(obs_size, hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_size, hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_size, out_dims),\n",
        "        nn.Tanh()\n",
        "    )\n",
        "\n",
        "  def mu(self, x):\n",
        "    if isinstance(x, np.ndarray):\n",
        "      x = torch.from_numpy(x).to(device)\n",
        "    return self.net(x.float()) * self.max\n",
        "\n",
        "  def forward(self, x, epsilon=0.0):\n",
        "    mu = self.mu(x)\n",
        "    mu = mu + torch.normal(0, epsilon, mu.size(), device=mu.device)\n",
        "    action = torch.max(torch.min(mu, self.max), self.min)\n",
        "    return action\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dv2XzwmtB3r"
      },
      "source": [
        "#### Create the Deep Q-Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9Z8OviE8V1A"
      },
      "outputs": [],
      "source": [
        "class DQN(nn.Module):\n",
        "\n",
        "  def __init__(self, hidden_size, obs_size, out_dims):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(obs_size + out_dims, hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_size, hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_size, 1),\n",
        "    )\n",
        "\n",
        "  def forward(self, state, action):\n",
        "    if isinstance(state, np.ndarray):\n",
        "      state = torch.from_numpy(state).to(device)\n",
        "    if isinstance(action, np.ndarray):\n",
        "      action = torch.from_numpy(action).to(device)\n",
        "    in_vector = torch.hstack((state, action))\n",
        "    return self.net(in_vector.float())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8XxdqufquQK6"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "\n",
        "  def __init__(self, capacity):\n",
        "    self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.buffer)\n",
        "\n",
        "  def append(self, experience):\n",
        "    self.buffer.append(experience)\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    return random.sample(self.buffer, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "roQfFswgKAZC"
      },
      "outputs": [],
      "source": [
        "class RLDataset(IterableDataset):\n",
        "\n",
        "  def __init__(self, buffer, sample_size=400):\n",
        "    self.buffer = buffer\n",
        "    self.sample_size = sample_size\n",
        "\n",
        "  def __iter__(self):\n",
        "    for experience in self.buffer.sample(self.sample_size):\n",
        "      yield experience"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7URcCS7uQNc"
      },
      "outputs": [],
      "source": [
        "def polyak_average(net, target_net, tau=0.01):\n",
        "    for qp, tp in zip(net.parameters(), target_net.parameters()):\n",
        "        tp.data.copy_(tau * qp.data + (1 - tau) * tp.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtxZePn-uQQA"
      },
      "outputs": [],
      "source": [
        "class DDPG(LightningModule):\n",
        "\n",
        "  def __init__(self, env_name, capacity=500, batch_size=8192, actor_lr=1e-3,\n",
        "               critic_lr=1e-3, hidden_size=256, gamma=0.99, loss_fn=F.smooth_l1_loss,\n",
        "               optim=AdamW, eps_start=1.0, eps_end=0.2, eps_last_episode=500,\n",
        "               samples_per_epoch=10, tau=0.005):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.env = create_environment(env_name, num_envs=batch_size)\n",
        "    self.obs = self.env.reset()\n",
        "    self.videos = []\n",
        "\n",
        "    obs_size = self.env.observation_space.shape[1]\n",
        "    action_dims = self.env.action_space.shape[1]\n",
        "    max_action = self.env.action_space.high\n",
        "    min_action = self.env.action_space.low\n",
        "\n",
        "    self.q_net = DQN(hidden_size, obs_size, action_dims)\n",
        "    self.policy = GradientPolicy(hidden_size, obs_size, action_dims, min_action, max_action)\n",
        "\n",
        "    self.target_policy = copy.deepcopy(self.policy)\n",
        "    self.target_q_net = copy.deepcopy(self.q_net)\n",
        "\n",
        "    self.buffer = ReplayBuffer(capacity=capacity)\n",
        "\n",
        "    self.save_hyperparameters()\n",
        "\n",
        "    while len(self.buffer) < self.hparams.samples_per_epoch:\n",
        "      print(f\"{len(self.buffer)} samples in experience buffer. Filling...\")\n",
        "      self.play(epsilon=self.hparams.eps_start)\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def play(self, policy=None, epsilon=0.):\n",
        "    if policy:\n",
        "      action = policy(self.obs, epsilon=epsilon)\n",
        "    else:\n",
        "        action = torch.from_numpy(self.env.action_space.sample()).to(device)\n",
        "    next_obs, reward, done, info = self.env.step(action)\n",
        "    exp = (self.obs, action, reward, done, next_obs)\n",
        "    self.buffer.append(exp)\n",
        "    self.obs = next_obs\n",
        "    return reward.mean()\n",
        "\n",
        "  def forward(self, x):\n",
        "    output = self.policy(x)\n",
        "    return output\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    q_net_optimizer = self.hparams.optim(self.q_net.parameters(), lr=self.hparams.critic_lr)\n",
        "    policy_optimizer = self.hparams.optim(self.policy.parameters(), lr=self.hparams.actor_lr)\n",
        "    return [q_net_optimizer, policy_optimizer]\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    dataset = RLDataset(self.buffer, self.hparams.samples_per_epoch)\n",
        "    dataloader = DataLoader(\n",
        "        dataset=dataset,\n",
        "        batch_size=1,\n",
        "    )\n",
        "    return dataloader\n",
        "\n",
        "  def training_step(self, batch, batch_idx, optimizer_idx):\n",
        "    epsilon = max(\n",
        "        self.hparams.eps_end,\n",
        "        self.hparams.eps_start - self.current_epoch / self.hparams.eps_last_episode\n",
        "    )\n",
        "\n",
        "    mean_reward = self.play(policy=self.policy, epsilon=epsilon)\n",
        "    self.log(\"episode/mean_reward\", mean_reward)\n",
        "\n",
        "    states, actions, rewards, dones, next_states = map(torch.squeeze, batch)\n",
        "    rewards = rewards.unsqueeze(1)\n",
        "    dones = dones.unsqueeze(1).bool()\n",
        "\n",
        "    polyak_average(self.q_net, self.target_q_net, tau=self.hparams.tau)\n",
        "    polyak_average(self.policy, self.target_policy, tau=self.hparams.tau)\n",
        "\n",
        "    if optimizer_idx == 0:\n",
        "      state_action_values = self.q_net(states, actions)\n",
        "      next_state_values = self.target_q_net(next_states, self.target_policy.mu(next_states))\n",
        "      next_state_values[dones] = 0.0\n",
        "      expected_state_action_values = rewards + self.hparams.gamma * next_state_values\n",
        "      q_loss = self.hparams.loss_fn(state_action_values, expected_state_action_values)\n",
        "      self.log_dict({\"episode/Q-Loss\": q_loss})\n",
        "      return q_loss\n",
        "\n",
        "    elif optimizer_idx == 1:\n",
        "      mu = self.policy.mu(states)\n",
        "      policy_loss = - self.q_net(states, mu).mean()\n",
        "      self.log_dict({\"episode/Policy Loss\": policy_loss})\n",
        "      return policy_loss\n",
        "\n",
        "  def training_epoch_end(self, outputs):\n",
        "    if self.current_epoch % 1000 == 0:\n",
        "      video = test_env('ant', policy=algo.policy)\n",
        "      self.videos.append(video)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhveWEKiwdy-"
      },
      "outputs": [],
      "source": [
        "# Start tensorboard.\n",
        "!rm -r /content/lightning_logs/\n",
        "!rm -r /content/videos/\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/lightning_logs/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wr82W3E0uQSo"
      },
      "outputs": [],
      "source": [
        "algo = DDPG('ant')\n",
        "\n",
        "trainer = Trainer(\n",
        "    gpus=num_gpus,\n",
        "    max_epochs=10_000,\n",
        "    log_every_n_steps=10\n",
        ")\n",
        "\n",
        "trainer.fit(algo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBTeij99B0Bf"
      },
      "outputs": [],
      "source": [
        "algo.videos[2]"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zXVLEDQj2vhb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}