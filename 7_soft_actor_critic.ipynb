{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GcEjHjuwnXW"
      },
      "source": [
        "## Soft Actor-Critic (SAC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29QddByhtuOm"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "!apt-get install -y \\\n",
        "    libgl1-mesa-dev \\\n",
        "    libgl1-mesa-glx \\\n",
        "    libglew-dev \\\n",
        "    xvfb \\\n",
        "    libosmesa6-dev \\\n",
        "    software-properties-common \\\n",
        "    patchelf\n",
        "\n",
        "!pip install \\\n",
        "    free-mujoco-py \\\n",
        "    gym==0.25.2 \\\n",
        "    pytorch-lightning==1.6.0 \\\n",
        "    optuna \\\n",
        "    gym-robotics \\\n",
        "    pyvirtualdisplay \\\n",
        "    PyOpenGL \\\n",
        "    PyOpenGL-accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOSJl-X7zvs4"
      },
      "source": [
        "#### Setup virtual display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-Z6takfzqGk"
      },
      "outputs": [],
      "source": [
        "from pyvirtualdisplay import Display\n",
        "Display(visible=False, size=(1400, 900)).start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cz8DLleGz_TF"
      },
      "source": [
        "#### Import the necessary code libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cP5t6U7-nYoc"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import gym\n",
        "import torch\n",
        "import itertools\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from collections import deque, namedtuple\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import IterableDataset\n",
        "from torch.optim import AdamW\n",
        "from torch.distributions.normal import Normal\n",
        "\n",
        "from pytorch_lightning import LightningModule, Trainer\n",
        "\n",
        "from gym.wrappers import RecordVideo, RecordEpisodeStatistics, \\\n",
        "  FilterObservation, FlattenObservation\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "num_gpus = torch.cuda.device_count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_IrPlU1wwPx"
      },
      "outputs": [],
      "source": [
        "def display_video(episode=0):\n",
        "  video_file = open(f'/content/videos/rl-video-episode-{episode}.mp4', \"r+b\").read()\n",
        "  video_url = f\"data:video/mp4;base64,{b64encode(video_file).decode()}\"\n",
        "  return HTML(f\"<video width=600 controls><source src='{video_url}'></video>\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Yeo3s-QPnZH"
      },
      "source": [
        "#### Create the replay buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKGbkg_6uIGK"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "\n",
        "  def __init__(self, capacity):\n",
        "    self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.buffer)\n",
        "\n",
        "  def append(self, experience):\n",
        "    self.buffer.append(experience)\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    return random.sample(self.buffer, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtGko6LVQaJz"
      },
      "outputs": [],
      "source": [
        "class RLDataset(IterableDataset):\n",
        "\n",
        "  def __init__(self, buffer, sample_size=400):\n",
        "    self.buffer = buffer\n",
        "    self.sample_size = sample_size\n",
        "\n",
        "  def __iter__(self):\n",
        "    for experience in self.buffer.sample(self.sample_size):\n",
        "      yield experience"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihkyoL5WQgGm"
      },
      "source": [
        "#### Create the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6_QhEyBNSrM"
      },
      "outputs": [],
      "source": [
        "def create_environment(name):\n",
        "  env = gym.make(name)\n",
        "  env = FilterObservation(env, ['observation', 'desired_goal'])\n",
        "  env = FlattenObservation(env)\n",
        "  env = RecordVideo(env, video_folder='./videos', episode_trigger=lambda x: x % 50 == 0)\n",
        "  env = RecordEpisodeStatistics(env)\n",
        "  return env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8fKGgFzQ4EX"
      },
      "source": [
        "#### Update the target network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-q-OJaPnBvKf"
      },
      "outputs": [],
      "source": [
        "def polyak_average(net, target_net, tau=0.01):\n",
        "    for qp, tp in zip(net.parameters(), target_net.parameters()):\n",
        "        tp.data.copy_(tau * qp.data + (1 - tau) * tp.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KipG5q0vAKi"
      },
      "source": [
        "#### Create the Deep Q-Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_l-HLfxtunM"
      },
      "outputs": [],
      "source": [
        "class DQN(nn.Module):\n",
        "\n",
        "  def __init__(self, hidden_size, obs_size, out_dims):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(obs_size + out_dims, hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_size, hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_size, 1),\n",
        "    )\n",
        "\n",
        "  def forward(self, state, action):\n",
        "    if isinstance(state, np.ndarray):\n",
        "      state = torch.from_numpy(state).to(device)\n",
        "    if isinstance(action, np.ndarray):\n",
        "      action = torch.from_numpy(action).to(device)\n",
        "    in_vector = torch.hstack((state, action))\n",
        "    return self.net(in_vector.float())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIkXBOhtug5p"
      },
      "source": [
        "#### Create the gradient policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "noMyzZ5DyOdQ"
      },
      "outputs": [],
      "source": [
        "class GradientPolicy(nn.Module):\n",
        "\n",
        "  def __init__(self, hidden_size, obs_size, out_dims, max):\n",
        "    super().__init__()\n",
        "\n",
        "    self.max = torch.from_numpy(max).to(device)\n",
        "\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(obs_size, hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_size, hidden_size),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "    self.linear_mu = nn.Linear(hidden_size, out_dims)\n",
        "    self.linear_std = nn.Linear(hidden_size, out_dims)\n",
        "    # self.linear_log_std = nn.Linear(hidden_size, out_dims)\n",
        "\n",
        "  def forward(self, obs):\n",
        "    if isinstance(obs, np.ndarray):\n",
        "      obs = torch.from_numpy(obs).to(device)\n",
        "    x = self.net(obs.float())\n",
        "    mu = self.linear_mu(x)\n",
        "    std = self.linear_std(x)\n",
        "    std = F.softplus(std) + 1e-3\n",
        "\n",
        "    dist = Normal(mu, std)\n",
        "    action = dist.rsample()\n",
        "    log_prob = dist.log_prob(action)\n",
        "    log_prob = log_prob.sum(dim=-1, keepdim=True)\n",
        "    log_prob -= (2* (np.log(2) - action - F.softplus(-2*action))).sum(dim=-1, keepdim=True)\n",
        "\n",
        "    action = torch.tanh(action) * self.max\n",
        "    return action, log_prob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTZH7WwlvKQ8"
      },
      "source": [
        "#### Soft actor-critic algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZcn2-X4yOfz"
      },
      "outputs": [],
      "source": [
        "class SAC(LightningModule):\n",
        "\n",
        "  def __init__(self, env_name, capacity=100_000, batch_size=256, lr=1e-3,\n",
        "               hidden_size=256, gamma=0.99, loss_fn=F.smooth_l1_loss, optim=AdamW,\n",
        "               samples_per_epoch=1_000, tau=0.05, alpha=0.02, epsilon=0.05):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.env = create_environment(env_name)\n",
        "\n",
        "    obs_size = self.env.observation_space.shape[0]\n",
        "    action_dims = self.env.action_space.shape[0]\n",
        "    max_action = self.env.action_space.high\n",
        "\n",
        "    self.q_net1 = DQN(hidden_size, obs_size, action_dims)\n",
        "    self.q_net2 = DQN(hidden_size, obs_size, action_dims)\n",
        "    self.policy = GradientPolicy(hidden_size, obs_size, action_dims, max_action)\n",
        "\n",
        "    self.target_policy = copy.deepcopy(self.policy)\n",
        "    self.target_q_net1 = copy.deepcopy(self.q_net1)\n",
        "    self.target_q_net2 = copy.deepcopy(self.q_net2)\n",
        "\n",
        "    self.buffer = ReplayBuffer(capacity=capacity)\n",
        "\n",
        "    self.save_hyperparameters()\n",
        "\n",
        "    while len(self.buffer) < self.hparams.samples_per_epoch:\n",
        "      print(f\"{len(self.buffer)} samples in experience buffer. Filling...\")\n",
        "      self.play_episodes()\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def play_episodes(self, policy=None):\n",
        "      obs = self.env.reset()\n",
        "      done = False\n",
        "\n",
        "      while not done:\n",
        "        if policy and random.random() > self.hparams.epsilon:\n",
        "          action, _ = self.policy(obs)\n",
        "          action = action.cpu().numpy()\n",
        "        else:\n",
        "          action = self.env.action_space.sample()\n",
        "\n",
        "        next_obs, reward, done, info = self.env.step(action)\n",
        "        exp = (obs, action, reward, done, next_obs)\n",
        "        self.buffer.append(exp)\n",
        "        obs = next_obs\n",
        "\n",
        "  def forward(self, x):\n",
        "    output = self.policy(x)\n",
        "    return output\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    q_net_parameters = itertools.chain(self.q_net1.parameters(), self.q_net2.parameters())\n",
        "    q_net_optimizer = self.hparams.optim(q_net_parameters, lr=self.hparams.lr)\n",
        "    policy_optimizer = self.hparams.optim(self.policy.parameters(), lr=self.hparams.lr)\n",
        "    return [q_net_optimizer, policy_optimizer]\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    dataset = RLDataset(self.buffer, self.hparams.samples_per_epoch)\n",
        "    dataloader = DataLoader(\n",
        "        dataset=dataset,\n",
        "        batch_size=self.hparams.batch_size,\n",
        "    )\n",
        "    return dataloader\n",
        "\n",
        "  def training_step(self, batch, batch_idx, optimizer_idx):\n",
        "    states, actions, rewards, dones, next_states = batch\n",
        "    rewards = rewards.unsqueeze(1)\n",
        "    dones = dones.unsqueeze(1)\n",
        "\n",
        "    if optimizer_idx == 0:\n",
        "\n",
        "      action_values1 = self.q_net1(states, actions)\n",
        "      action_values2 = self.q_net2(states, actions)\n",
        "\n",
        "      target_actions, target_log_probs = self.target_policy(next_states)\n",
        "\n",
        "      next_action_values = torch.min(\n",
        "          self.target_q_net1(next_states, target_actions),\n",
        "          self.target_q_net2(next_states, target_actions)\n",
        "      )\n",
        "      next_action_values[dones] = 0.0\n",
        "\n",
        "      expected_action_values = rewards + self.hparams.gamma * (next_action_values - self.hparams.alpha * target_log_probs)\n",
        "\n",
        "      q_loss1 = self.hparams.loss_fn(action_values1, expected_action_values)\n",
        "      q_loss2 = self.hparams.loss_fn(action_values2, expected_action_values)\n",
        "\n",
        "      q_loss_total = q_loss1 + q_loss2\n",
        "      self.log(\"episode/Q-Loss\", q_loss_total)\n",
        "      return q_loss_total\n",
        "\n",
        "    elif optimizer_idx == 1:\n",
        "\n",
        "      actions, log_probs = self.policy(states)\n",
        "\n",
        "      action_values = torch.min(\n",
        "          self.q_net1(states, actions),\n",
        "          self.q_net2(states, actions)\n",
        "      )\n",
        "\n",
        "      policy_loss = (self.hparams.alpha * log_probs - action_values).mean()\n",
        "      self.log(\"episode/Policy Loss\", policy_loss)\n",
        "      return policy_loss\n",
        "\n",
        "  def training_epoch_end(self, training_step_outputs):\n",
        "    self.play_episodes(policy=self.policy)\n",
        "\n",
        "    polyak_average(self.q_net1, self.target_q_net1, tau=self.hparams.tau)\n",
        "    polyak_average(self.q_net2, self.target_q_net2, tau=self.hparams.tau)\n",
        "    polyak_average(self.policy, self.target_policy, tau=self.hparams.tau)\n",
        "\n",
        "    self.log(\"episode/episode_return\", self.env.return_queue[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_wtApvqyOh5"
      },
      "outputs": [],
      "source": [
        "# Start tensorboard.\n",
        "!rm -r /content/lightning_logs/\n",
        "!rm -r /content/videos/\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/lightning_logs/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNu1BWZBQGTg"
      },
      "outputs": [],
      "source": [
        "algo = SAC('FetchReachDense-v1', lr=1e-3, alpha=0.002, tau=0.1)\n",
        "\n",
        "trainer = Trainer(\n",
        "    gpus=num_gpus,\n",
        "    max_epochs=2000,\n",
        "    log_every_n_steps=10\n",
        ")\n",
        "\n",
        "trainer.fit(algo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pbok-MwMQGWZ"
      },
      "outputs": [],
      "source": [
        "display_video(episode=2000)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LziLsoYGjuAs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}