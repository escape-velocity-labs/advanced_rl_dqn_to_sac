{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCZmQKLpBtFQ"
      },
      "source": [
        "## DQN for continuous action spaces: Normalized Advantage Function (NAF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5s0R3qxBnTN"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "!apt-get update && apt-get install -y xvfb\n",
        "!pip install swig\n",
        "!pip install gym[box2d]==0.23.1 pytorch-lightning==1.6.0 pyvirtualdisplay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOSJl-X7zvs4"
      },
      "source": [
        "#### Setup virtual display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-Z6takfzqGk"
      },
      "outputs": [],
      "source": [
        "from pyvirtualdisplay import Display\n",
        "Display(visible=False, size=(1400, 900)).start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cz8DLleGz_TF"
      },
      "source": [
        "#### Import the necessary code libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cP5t6U7-nYoc"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import gym\n",
        "import random\n",
        "import torch\n",
        "\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from collections import deque, namedtuple\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import IterableDataset\n",
        "from torch.optim import AdamW\n",
        "\n",
        "from pytorch_lightning import LightningModule, Trainer\n",
        "\n",
        "from gym.wrappers import RecordVideo, RecordEpisodeStatistics\n",
        "\n",
        "\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "num_gpus = torch.cuda.device_count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_IrPlU1wwPx"
      },
      "outputs": [],
      "source": [
        "def display_video(episode=0):\n",
        "  video_file = open(f'/content/videos/rl-video-episode-{episode}.mp4', \"r+b\").read()\n",
        "  video_url = f\"data:video/mp4;base64,{b64encode(video_file).decode()}\"\n",
        "  return HTML(f\"<video width=600 controls><source src='{video_url}'></video>\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMKrYHMnFISO"
      },
      "source": [
        "#### Create the Deep Q-Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGAOZKhbBu_z"
      },
      "outputs": [],
      "source": [
        "class NafDQN(nn.Module):\n",
        "    \n",
        "  def __init__(self, hidden_size, obs_size, action_dims, max_action):\n",
        "    super().__init__()\n",
        "    self.action_dims = action_dims\n",
        "    self.max_action = torch.from_numpy(max_action).to(device)\n",
        "    self.net = nn.Sequential(\n",
        "      nn.Linear(obs_size, hidden_size),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(hidden_size, hidden_size),\n",
        "      nn.ReLU(),   \n",
        "    )\n",
        "    self.linear_mu = nn.Linear(hidden_size, action_dims)\n",
        "    self.linear_value = nn.Linear(hidden_size, 1)\n",
        "    self.linear_matrix = nn.Linear(hidden_size, int(action_dims * (action_dims + 1) / 2))\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def mu(self, x):\n",
        "    x = self.net(x)\n",
        "    x = self.linear_mu(x)\n",
        "    x = torch.tanh(x) * self.max_action\n",
        "    return x\n",
        "  \n",
        "  @torch.no_grad()\n",
        "  def value(self, x):\n",
        "    x = self.net(x)\n",
        "    x = self.linear_value(x)\n",
        "    return x\n",
        "\n",
        "  def forward(self, x, a):\n",
        "    x = self.net(x)\n",
        "    mu = torch.tanh(self.linear_mu(x)) * self.max_action\n",
        "    value = self.linear_value(x)\n",
        "    matrix = torch.tanh(self.linear_matrix(x))\n",
        "    \n",
        "    L = torch.zeros((x.shape[0], self.action_dims, self.action_dims)).to(device)\n",
        "    tril_indices = torch.tril_indices(row=self.action_dims, col=self.action_dims, offset=0).to(device)\n",
        "\n",
        "    L[:, tril_indices[0], tril_indices[1]] = matrix\n",
        "    L.diagonal(dim1=1,dim2=2).exp_()\n",
        "    P = L * L.transpose(2, 1)\n",
        "    \n",
        "    u_mu = (a-mu).unsqueeze(dim=1)\n",
        "    u_mu_t = u_mu.transpose(1, 2)\n",
        "    \n",
        "    adv = - 1/2 * u_mu @ P @ u_mu_t\n",
        "    adv = adv.squeeze(dim=-1)\n",
        "    return value + adv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hziRGjJ9Pkv1"
      },
      "source": [
        "#### Create the policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5B223HLzBvCx"
      },
      "outputs": [],
      "source": [
        "def noisy_policy(state, env, net, epsilon=0.0):\n",
        "  state = torch.tensor([state]).to(device)\n",
        "  amin = torch.from_numpy(env.action_space.low).to(device)\n",
        "  amax = torch.from_numpy(env.action_space.high).to(device)\n",
        "  mu = net.mu(state)\n",
        "  mu = mu + torch.normal(0, epsilon, mu.size(), device=device)\n",
        "  action = mu.clamp(amin, amax)\n",
        "  action = action.squeeze().cpu().numpy()\n",
        "  return action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Yeo3s-QPnZH"
      },
      "source": [
        "#### Create the replay buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fw-77TRyBvHz"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "\n",
        "  def __init__(self, capacity):\n",
        "    self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.buffer)\n",
        "  \n",
        "  def append(self, experience):\n",
        "    self.buffer.append(experience)\n",
        "  \n",
        "  def sample(self, batch_size):\n",
        "    return random.sample(self.buffer, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtGko6LVQaJz"
      },
      "outputs": [],
      "source": [
        "class RLDataset(IterableDataset):\n",
        "\n",
        "  def __init__(self, buffer, sample_size=400):\n",
        "    self.buffer = buffer\n",
        "    self.sample_size = sample_size\n",
        "  \n",
        "  def __iter__(self):\n",
        "    for experience in self.buffer.sample(self.sample_size):\n",
        "      yield experience"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihkyoL5WQgGm"
      },
      "source": [
        "#### Create the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XQlZy9J-vjt"
      },
      "outputs": [],
      "source": [
        "class RepeatActionWrapper(gym.Wrapper):\n",
        "  def __init__(self, env, n):\n",
        "    super().__init__(env)\n",
        "    self.env = env\n",
        "    self.n = n\n",
        "      \n",
        "  def step(self, action):\n",
        "    done = False\n",
        "    total_reward = 0.0\n",
        "    for _ in range(self.n):\n",
        "      next_state, reward, done, info = self.env.step(action)\n",
        "      total_reward += reward\n",
        "      if done:\n",
        "        break\n",
        "    return next_state, total_reward, done, info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZ3h8CCOQjGL"
      },
      "outputs": [],
      "source": [
        "def create_environment(name):\n",
        "  env = gym.make(name)\n",
        "  env = RecordVideo(env, video_folder='./videos', episode_trigger=lambda x: x % 50 == 0)\n",
        "  env = RepeatActionWrapper(env, n=8)\n",
        "  env = RecordEpisodeStatistics(env)\n",
        "  return env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8fKGgFzQ4EX"
      },
      "source": [
        "#### Update the target network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-q-OJaPnBvKf"
      },
      "outputs": [],
      "source": [
        "def polyak_average(net, target_net, tau=0.01):\n",
        "    for qp, tp in zip(net.parameters(), target_net.parameters()):\n",
        "        tp.data.copy_(tau * qp.data + (1 - tau) * tp.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8de_OtyR1oJ"
      },
      "source": [
        "#### Create the Deep Q-Learning algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-tOW8KgBvNZ"
      },
      "outputs": [],
      "source": [
        "class NAFDeepQLearning(LightningModule):\n",
        "                             \n",
        "  def __init__(self, env_name, policy=noisy_policy, capacity=100_000, \n",
        "               batch_size=256, lr=1e-4, hidden_size=512, gamma=0.99, \n",
        "               loss_fn=F.smooth_l1_loss, optim=AdamW, eps_start=2.0, eps_end=0.2, \n",
        "               eps_last_episode=1_000, samples_per_epoch=1_000, tau=0.01):\n",
        "\n",
        "    super().__init__()\n",
        "    self.env = create_environment(env_name)\n",
        "\n",
        "    obs_size = self.env.observation_space.shape[0]\n",
        "    action_dims = self.env.action_space.shape[0]\n",
        "    max_action = self.env.action_space.high\n",
        "\n",
        "    self.q_net = NafDQN(hidden_size, obs_size, action_dims, max_action).to(device)\n",
        "    self.target_q_net = copy.deepcopy(self.q_net)\n",
        "    self.policy = policy\n",
        "\n",
        "    self.buffer = ReplayBuffer(capacity=capacity)\n",
        "\n",
        "    self.save_hyperparameters()\n",
        "\n",
        "    while len(self.buffer) < self.hparams.samples_per_epoch:\n",
        "\n",
        "      print(f\"{len(self.buffer)} samples in experience buffer. Filling...\")\n",
        "      self.play_episode(epsilon=self.hparams.eps_start)\n",
        "  \n",
        "  @torch.no_grad()\n",
        "  def play_episode(self, policy=None, epsilon=0.):\n",
        "    obs = self.env.reset()\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "      if policy:\n",
        "        action = policy(obs, self.env, self.q_net, epsilon=epsilon)\n",
        "      else:\n",
        "        action = self.env.action_space.sample()\n",
        "        \n",
        "      next_obs, reward, done, info = self.env.step(action)\n",
        "      exp = (obs, action, reward, done, next_obs)\n",
        "      self.buffer.append(exp)\n",
        "      obs = next_obs\n",
        "  \n",
        "  def forward(self, x):\n",
        "    output = self.q_net(x)\n",
        "    return output\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    q_net_optimizer = self.hparams.optim(self.q_net.parameters(), lr=self.hparams.lr)\n",
        "    return [q_net_optimizer]\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    dataset = RLDataset(self.buffer, self.hparams.samples_per_epoch)\n",
        "    dataloader = DataLoader(\n",
        "        dataset=dataset,\n",
        "        batch_size=self.hparams.batch_size,\n",
        "    )\n",
        "    return dataloader\n",
        "\n",
        "  def training_step(self, batch, batch_idx):\n",
        "    states, actions, rewards, dones, next_states = batch\n",
        "    rewards = rewards.unsqueeze(1)\n",
        "    dones = dones.unsqueeze(1)\n",
        "\n",
        "    action_values = self.q_net(states, actions)\n",
        "\n",
        "    next_state_values = self.target_q_net.value(next_states)\n",
        "    next_state_values[dones] = 0.0\n",
        "    \n",
        "    target = rewards + self.hparams.gamma * next_state_values\n",
        "\n",
        "    loss = self.hparams.loss_fn(action_values, target)\n",
        "    self.log('episode/MSE Loss', loss)\n",
        "    return loss\n",
        "\n",
        "  def training_epoch_end(self, training_step_outputs):\n",
        "\n",
        "    epsilon = max(\n",
        "        self.hparams.eps_end,\n",
        "        self.hparams.eps_start - self.current_epoch / self.hparams.eps_last_episode\n",
        "    )\n",
        "\n",
        "    self.play_episode(policy=self.policy, epsilon=epsilon)\n",
        "    \n",
        "    polyak_average(self.q_net, self.target_q_net, tau=self.hparams.tau)\n",
        "    \n",
        "    self.log(\"episode/Return\", self.env.return_queue[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCusNrF-SPHP"
      },
      "source": [
        "#### Purge logs and run the visualization tool (Tensorboard)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOhCyJgTBvQR"
      },
      "outputs": [],
      "source": [
        "# Start tensorboard.\n",
        "!rm -r /content/lightning_logs/\n",
        "!rm -r /content/videos/\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/lightning_logs/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-fecCQPSVD6"
      },
      "source": [
        "#### Train the policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yY3-mV12BvTK"
      },
      "outputs": [],
      "source": [
        "algo = NAFDeepQLearning('LunarLanderContinuous-v2')\n",
        "\n",
        "trainer = Trainer(\n",
        "    gpus=num_gpus, \n",
        "    max_epochs=10_000\n",
        ")\n",
        "\n",
        "trainer.fit(algo)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aURap6K8SXLh"
      },
      "source": [
        "#### Check the resulting policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0q9_nEOBvV3"
      },
      "outputs": [],
      "source": [
        "display_video(episode=4300)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
