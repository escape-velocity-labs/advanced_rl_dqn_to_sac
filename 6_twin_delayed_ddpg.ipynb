{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StmTP2EXwnUr"
      },
      "source": [
        "## Twin Delayed DDPG (TD3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5s0R3qxBnTN"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "!apt-get install -y xvfb\n",
        "\n",
        "!pip install gym==0.23 \\\n",
        "    pytorch-lightning==1.6.0 \\\n",
        "    pyvirtualdisplay \\\n",
        "    brax==0.10.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOSJl-X7zvs4"
      },
      "source": [
        "#### Setup virtual display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-Z6takfzqGk"
      },
      "outputs": [],
      "source": [
        "from pyvirtualdisplay import Display\n",
        "Display(visible=False, size=(1400, 900)).start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cz8DLleGz_TF"
      },
      "source": [
        "#### Import the necessary code libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cP5t6U7-nYoc"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import gym\n",
        "import torch\n",
        "import random\n",
        "import functools\n",
        "import itertools\n",
        "\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from collections import deque, namedtuple\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import IterableDataset\n",
        "from torch.optim import AdamW\n",
        "\n",
        "from pytorch_lightning import LightningModule, Trainer\n",
        "\n",
        "import brax\n",
        "from brax import envs\n",
        "from brax.envs.wrappers import gym as gym_wrapper\n",
        "from brax.envs.wrappers import torch as torch_wrapper\n",
        "\n",
        "from brax.io import html\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "num_gpus = torch.cuda.device_count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_IrPlU1wwPx"
      },
      "outputs": [],
      "source": [
        "def display_video(episode=0):\n",
        "    video_file = open(f'/content/videos/rl-video-episode-{episode}.mp4', \"r+b\").read()\n",
        "    video_url = f\"data:video/mp4;base64,{b64encode(video_file).decode()}\"\n",
        "    return HTML(f\"<video width=600 controls><source src='{video_url}'></video>\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CvcCbQMmlkXr"
      },
      "outputs": [],
      "source": [
        "def create_environment(env_name, num_envs=256, episode_length=1000):\n",
        "    env = envs.create(env_name, batch_size=num_envs, episode_length=episode_length, backend='spring')\n",
        "    env = gym_wrapper.VectorGymWrapper(env)\n",
        "    env = torch_wrapper.TorchWrapper(env, device=device)\n",
        "    return env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjl0VTwemYtd"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def test_env(env_name, policy=None):\n",
        "    env = envs.create(env_name, episode_length=1000, backend='spring')\n",
        "    env = gym_wrapper.GymWrapper(env)\n",
        "    env = torch_wrapper.TorchWrapper(env, device=device)\n",
        "    ps_array = []\n",
        "    state = env.reset()\n",
        "    for i in range(1000):\n",
        "        if policy:\n",
        "            action = algo.policy.net(state.unsqueeze(0)).squeeze()\n",
        "        else:\n",
        "            action = torch.from_numpy(env.action_space.sample()).to(device)\n",
        "        state, _, _, _ = env.step(action)\n",
        "        ps_array.extend([env.unwrapped._state.pipeline_state]*5)\n",
        "    return HTML(html.render(env.unwrapped._env.sys, ps_array))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCe1SlnWmYvj"
      },
      "outputs": [],
      "source": [
        "test_env('ant')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dj6w5kLkz9DN"
      },
      "source": [
        "#### Create the gradient policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nW2PbJDmhNP"
      },
      "outputs": [],
      "source": [
        "class GradientPolicy(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size, obs_size, out_dims, min, max):\n",
        "        super().__init__()\n",
        "        self.min = torch.from_numpy(min).to(device)\n",
        "        self.max = torch.from_numpy(max).to(device)\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(obs_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, out_dims),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def mu(self, x):\n",
        "        if isinstance(x, np.ndarray):\n",
        "            x = torch.from_numpy(x).to(device)\n",
        "        return self.net(x.float()) * self.max\n",
        "\n",
        "    def forward(self, x, epsilon=0.0, noise_clip=None):\n",
        "        mu = self.mu(x)\n",
        "        noise = torch.normal(0, epsilon, mu.size(), device=mu.device)\n",
        "        if noise_clip is not None:\n",
        "            noise = torch.clamp(noise, - noise_clip, noise_clip)\n",
        "        mu = mu + noise\n",
        "        action = torch.max(torch.min(mu, self.max), self.min)\n",
        "        return action\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdELkIwXmp5O"
      },
      "source": [
        "#### Create the Deep Q-Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZvjEJuhmhQE"
      },
      "outputs": [],
      "source": [
        "class DQN(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size, obs_size, out_dims):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(obs_size + out_dims, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        if isinstance(state, np.ndarray):\n",
        "            state = torch.from_numpy(state).to(device)\n",
        "        if isinstance(action, np.ndarray):\n",
        "            action = torch.from_numpy(action).to(device)\n",
        "        in_vector = torch.hstack((state, action))\n",
        "        return self.net(in_vector.float())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hYzmIGWmuu_"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def append(self, experience):\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.buffer, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBDYXvaVmhS1"
      },
      "outputs": [],
      "source": [
        "class RLDataset(IterableDataset):\n",
        "\n",
        "    def __init__(self, buffer, sample_size=400):\n",
        "        self.buffer = buffer\n",
        "        self.sample_size = sample_size\n",
        "\n",
        "    def __iter__(self):\n",
        "        for experience in self.buffer.sample(self.sample_size):\n",
        "            yield experience"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1GI3XMZmhVs"
      },
      "outputs": [],
      "source": [
        "def polyak_average(net, target_net, tau=0.01):\n",
        "    for qp, tp in zip(net.parameters(), target_net.parameters()):\n",
        "        tp.data.copy_(tau * qp.data + (1 - tau) * tp.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZBeQSNvzYRk"
      },
      "source": [
        "Create the Deep Q-Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SKFUd7jAf0Ej"
      },
      "outputs": [],
      "source": [
        "class TD3(LightningModule):\n",
        "\n",
        "    def __init__(self, env_name, capacity=500, batch_size=8192, actor_lr=1e-3,\n",
        "                 critic_lr=1e-3, hidden_size=256, gamma=0.99, loss_fn=F.smooth_l1_loss,\n",
        "                 optim=AdamW, eps_start=1.0, eps_end=0.2, eps_last_episode=500,\n",
        "                 samples_per_epoch=10, tau=0.005):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.env = create_environment(env_name, num_envs=batch_size)\n",
        "        self.obs = self.env.reset()\n",
        "        self.videos = []\n",
        "\n",
        "        obs_size = self.env.observation_space.shape[1]\n",
        "        action_dims = self.env.action_space.shape[1]\n",
        "        max_action = self.env.action_space.high\n",
        "        min_action = self.env.action_space.low\n",
        "\n",
        "\n",
        "        self.q_net1 = DQN(hidden_size, obs_size, action_dims).to(device)\n",
        "        self.q_net2 = DQN(hidden_size, obs_size, action_dims).to(device)\n",
        "        self.policy = GradientPolicy(hidden_size, obs_size, action_dims, min_action, max_action).to(device)\n",
        "\n",
        "        self.target_policy = copy.deepcopy(self.policy)\n",
        "        self.target_q_net1 = copy.deepcopy(self.q_net1)\n",
        "        self.target_q_net2 = copy.deepcopy(self.q_net2)\n",
        "\n",
        "        self.buffer = ReplayBuffer(capacity=capacity)\n",
        "\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        while len(self.buffer) < self.hparams.samples_per_epoch:\n",
        "            print(f\"{len(self.buffer)} samples in experience buffer. Filling...\")\n",
        "            self.play(epsilon=self.hparams.eps_start)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def play(self, policy=None, epsilon=0.):\n",
        "        if policy:\n",
        "            action = policy(self.obs, epsilon=epsilon)\n",
        "        else:\n",
        "            action = torch.from_numpy(self.env.action_space.sample()).to(device)\n",
        "        next_obs, reward, done, info = self.env.step(action)\n",
        "        exp = (self.obs, action, reward, done, next_obs)\n",
        "        self.buffer.append(exp)\n",
        "        self.obs = next_obs\n",
        "        return reward.mean()\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.policy.mu(x)\n",
        "        return output\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        q_net_parameters = itertools.chain(self.q_net1.parameters(), self.q_net2.parameters())\n",
        "        q_net_optimizer = self.hparams.optim(q_net_parameters, lr=self.hparams.critic_lr)\n",
        "        policy_optimizer = self.hparams.optim(self.policy.parameters(), lr=self.hparams.actor_lr)\n",
        "        return [q_net_optimizer, policy_optimizer]\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        dataset = RLDataset(self.buffer, self.hparams.samples_per_epoch)\n",
        "        dataloader = DataLoader(\n",
        "            dataset=dataset,\n",
        "            batch_size=1\n",
        "        )\n",
        "        return dataloader\n",
        "\n",
        "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
        "        epsilon = max(\n",
        "            self.hparams.eps_end,\n",
        "            self.hparams.eps_start - self.current_epoch / self.hparams.eps_last_episode\n",
        "        )\n",
        "\n",
        "        mean_reward = self.play(policy=self.policy, epsilon=epsilon)\n",
        "        self.log('episode/mean_reward', mean_reward)\n",
        "\n",
        "        polyak_average(self.q_net1, self.target_q_net1, tau=self.hparams.tau)\n",
        "        polyak_average(self.q_net2, self.target_q_net2, tau=self.hparams.tau)\n",
        "        polyak_average(self.policy, self.target_policy, tau=self.hparams.tau)\n",
        "\n",
        "        states, actions, rewards, dones, next_states = map(torch.squeeze, batch)\n",
        "        rewards = rewards.unsqueeze(1)\n",
        "        dones = dones.unsqueeze(1).bool()\n",
        "\n",
        "        if optimizer_idx == 0:\n",
        "            action_values1 = self.q_net1(states, actions)\n",
        "            action_values2 = self.q_net2(states, actions)\n",
        "\n",
        "            next_actions = self.target_policy(next_states, epsilon=epsilon, noise_clip=0.05)\n",
        "\n",
        "            next_action_values = torch.min(\n",
        "                self.target_q_net1(next_states, next_actions),\n",
        "                self.target_q_net2(next_states, next_actions),\n",
        "            )\n",
        "\n",
        "            next_action_values[dones] = 0.0\n",
        "\n",
        "            expected_action_values = rewards + self.hparams.gamma * next_action_values\n",
        "\n",
        "            q_loss1 = self.hparams.loss_fn(action_values1, expected_action_values)\n",
        "            q_loss2 = self.hparams.loss_fn(action_values2, expected_action_values)\n",
        "            total_loss = q_loss1 + q_loss2\n",
        "            self.log(\"episode/Q-Loss\", total_loss)\n",
        "            return total_loss\n",
        "\n",
        "        elif optimizer_idx == 1 and batch_idx % 2 == 0:\n",
        "            mu = self.policy.mu(states)\n",
        "            policy_loss = - self.q_net1(states, mu).mean()\n",
        "            self.log(\"episode/Policy Loss\", policy_loss)\n",
        "            return policy_loss\n",
        "\n",
        "    def training_epoch_end(self, training_step_outputs):\n",
        "        if self.current_epoch % 1000 == 0:\n",
        "            video = test_env('ant', policy=self.policy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1b2qnsACnz4"
      },
      "outputs": [],
      "source": [
        "# Start tensorboard.\n",
        "!rm -r /content/lightning_logs/\n",
        "!rm -r /content/videos/\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/lightning_logs/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3rn7R1Sff0HS"
      },
      "outputs": [],
      "source": [
        "algo = TD3('ant')\n",
        "\n",
        "trainer = Trainer(\n",
        "    gpus=num_gpus,\n",
        "    max_epochs=5_000,\n",
        "    log_every_n_steps=10\n",
        ")\n",
        "\n",
        "trainer.fit(algo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nk8UIOk2_rQp"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}